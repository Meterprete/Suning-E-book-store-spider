# Suning-E-book-store-spider
**ä¸‹è½½æ–¹å¼æä¾›ä¸¤ç§ï¼š**
 1. [githubä¸‹è½½](https://github.com/Meterprete/Suning-E-book-store-spider.git)
 2. [è…¾è®¯å¾®äº‘ä¸‹è½½ã€å¯†ç ï¼š54250pã€‘](https://share.weiyun.com/5r4YMjA)

**è‹å®æ˜“è´­ä¹¦åŸçˆ¬è™«æŠ“å–çš„å†…å®¹**
 - å›¾ä¹¦ä»·æ ¼
 - å›¾ä¹¦æ ‡é¢˜
 - å›¾ä¹¦è´­ä¹°è¯¦æƒ…é¡µé“¾æ¥
 - å›¾ä¹¦åˆ†ç±»
 - å›¾ä¹¦ä½œè€…
 - å›¾ä¹¦å‡ºç‰ˆç¤¾ä»¥åŠå‡ºç‰ˆæ—¥æœŸ

**ç›´æ¥åˆ‡å…¥æ­£é¢˜ï¼Œé‡ç‚¹æ˜¯å›¾ä¹¦ä»·æ ¼çš„æ„é€ ï¼Œæ‰€ä»¥ï¼Œå¼€å§‹å°±å…ˆè§£å†³å›¾ä¹¦ä»·æ ¼è¯·æ±‚çš„urlæ„é€ ï¼š**
è¿™é‡Œå…ˆåˆ—ä¸€ä¸‹æœ¬æ–‡é‡Œåˆ†æçš„é¡µé¢æœ‰å“ªå‡ ä¸ªï¼ˆå¦‚ä¸‹ï¼‰ï¼š
ï¼ˆ1ï¼‰[åˆå§‹åˆ†æé¡µé¢ã€å›¾ä¹¦ç±»åˆ«åˆ†ç±»é¡µã€‘](https://book.suning.com/)
ï¼ˆ2ï¼‰[å›¾ä¹¦ç›®å½•é¡µé¢ã€è¿™é‡Œæ‹¿å…¶ä¸­çš„â€`å°è¯´---->é­”å¹»`â€œç±»æ¥åˆ†æã€‘](https://list.suning.com/1-502697-0.html)
ï¼ˆ3ï¼‰[å›¾ä¹¦è¯¦æƒ…é¡µã€å›¾ä¹¦è´­ä¹°è¯¦æƒ…é¡µã€‘](https://product.suning.com/0070768206/11188777107.html?safp=d488778a.10038.resultsRblock.12&safc=prd.3.ssdln_502697_pro_pic01-1_0_0_11188777107_0070768206)

é¦–å…ˆï¼Œä»[å›¾ä¹¦ç›®å½•é¡µé¢](https://list.suning.com/1-502697-0.html)è¿™ä¸ªé¡µé¢æ¥çœ‹ï¼Œä»·æ ¼åœ¨è¿™é‡Œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»·æ ¼çš„è·å–å¹¶å®¹æ˜“ï¼Œé¦–å…ˆæ˜¯å’Œå…¶ä»–ä¿¡æ¯éƒ½ä¸ä¸€æ ·ï¼Œå…¶ä»–ä¿¡æ¯éƒ½å¯ä»¥ä»[å›¾ä¹¦ç›®å½•é¡µé¢](https://list.suning.com/1-502697-0.html)æ¥ç›´æ¥æå–ï¼Œç„¶è€Œä»·æ ¼è¿™ä¸ªæ•°æ®ï¼Œæ˜¯è¯·æ±‚å¦å¤–çš„urlç”Ÿæˆçš„ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209095622674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
å¾—å‡ºç»“è®ºä»¥åï¼Œå¼€å§‹å¯»æ‰¾è¯·æ±‚çš„urllåœ¨å“ªï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¨å±€æœç´¢ç¡®å®æ‰¾åˆ°äº†ï¼Œä½†æ˜¯è¿™å¥½åƒè´¼éš¾æ–­å®šä»·æ ¼åˆ°åº•æ˜¯æ€ä¹ˆä¸ªæ„é€ æ³•ï¼Œå¹¶ä¸”è¿™ä¸ªurlè¿”å›çš„jsonæ•°æ®ç«Ÿç„¶æ˜¯5æ¡ï¼Œæ¯æ¡å¯¹åº”ä¸€ä¸ªä»·æ ¼ï¼Œå¹¶ä¸”é¡µé¢ä¸­å„å•†å“ä»·æ ¼çš„å­˜æ”¾ä¹Ÿæ²¡æœ‰ä¸€ç‚¹è§„å¾‹ã€‚æœ‰ä¸€è¯´ä¸€ï¼Œå¦‚æœå¼€å§‹çš„æ—¶å€™ä»è¿™ä¸ªé¡µé¢ç›´æ¥åˆ†æä»·æ ¼æ€ä¹ˆæ„é€ å‡ºæ¥çš„ï¼Œé‚£å¯å°±çœŸçš„çäº†
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209094738355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
ç»“è®ºï¼Œä¸èƒ½ç›´æ¥ä»`[å›¾ä¹¦ç›®å½•é¡µé¢]`è¿™ä¸ªé¡µé¢åˆ†æä»·æ ¼è¯·æ±‚çš„urlæ˜¯ä»€ä¹ˆæ„é€ ã€‚è¿˜æœ‰ä¸€ä¸ªåœ°æ–¹ä¹Ÿè¯·æ±‚äº†å›¾ä¹¦çš„ä»·æ ¼çš„ä¿¡æ¯ï¼Œå°±æ˜¯[å›¾ä¹¦çš„è¯¦æƒ…é¡µ](https://product.suning.com/0070768206/11188777107.html?safp=d488778a.10038.resultsRblock.12&safc=prd.3.ssdln_502697_pro_pic01-1_0_0_11188777107_0070768206)ï¼Œæ¯ä¸ªè¯¦æƒ…é¡µå›¾ä¹¦ä»·æ ¼è¿”å›è‚¯å®šæ˜¯å”¯ä¸€çš„ï¼Œé¦–å…ˆæ˜¯å¥½åˆ¤æ–­ï¼Œå…¶æ¬¡æ˜¯ï¼Œå¾ˆæœ‰å¯èƒ½åˆ†æå‡ºurlä»€ä¹ˆæ„é€ æ–¹æ³•ã€‚æ•…ï¼Œæ¥ç€åˆ†æï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ‰¾åˆ°äº†è¯·æ±‚çš„urlï¼Œå¹¶ä¸”ä¹Ÿé”å®šäº†æ•°æ®åœ¨ä»€ä¹ˆä½ç½®ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209101103874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
é‚£ä¹ˆï¼Œå°±æ¥çœ‹çœ‹è¯·æ±‚å›¾ä¹¦ä»·æ ¼çš„urlé•¿ä»€ä¹ˆæ ·ï¼š
é•¿è¿™æ ·----->è´¼çŒ¥çï¼š![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209101509243.png)
é¦–å…ˆï¼Œæ®ç»éªŒæ¥çœ‹ï¼Œåé¢çš„`callback=pcData&_=1581214174334`ç›´æ¥åˆ æ‰ï¼Œæ²¡ä»€ä¹ˆç”¨ã€‚å¥½äº†ï¼Œä¸‹å›¾æ‰€ç¤ºï¼Œåˆ æ‰äº†ï¼Œé‚£ä¹ˆå‰©ä¸‹çš„åˆæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209101656645.png)
è¿™é‡Œï¼Œçœ‹äº†ä¸€ä¸‹åˆšåˆšè¯·æ±‚çš„[å›¾ä¹¦çš„è¯¦æƒ…é¡µ](https://product.suning.com/0070768206/11188777107.html?safp=d488778a.10038.resultsRblock.12&safc=prd.3.ssdln_502697_pro_pic01-1_0_0_11188777107_0070768206)çš„urlåœ°å€ï¼Œé‡Œé¢å°±æœ‰å‡ å’Œè¿™ä¸ªurlä¸­ä¸€æ ·çš„å‚æ•°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209102039636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
åˆ°è¿™ä¸€æ­¥ä¸ºæ­¢ï¼Œå°±å‰©ä¸‹å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å‚æ•°æœªçŸ¥äº†![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209102155706.png)
è¿™é‡Œé¦–å…ˆçš„æƒ³æ³•æ˜¯ï¼Œè¿™äº›å‚æ•°æ²¡ç”¨ï¼Œå¯ä»¥å»æ‰ï¼Œä¸å½±å“è¿”å›å€¼ã€‚
å®éªŒéªŒè¯ï¼Œè¿™äº›å‚æ•°æœ‰ä¸€äº›æ˜¯æœ‰ç”¨çš„ï¼Œä¸€äº›æ˜¯å¯ä»¥ç›´æ¥åˆ æ‰ä¸å½±å“è¿”å›ç»“æœçš„ã€‚è¿™é‡Œå°±ä¸ç»†è¯´äº†ï¼Œå¯ä»¥è‡ªå·±å»å°è¯•ï¼Œä¸‹å›¾æ‰€ç¤ºï¼Œæ˜¯å¾—åˆ°çš„æœ€ç®€çš„è¯·æ±‚ä»·æ ¼ä¿¡æ¯çš„urlï¼Œå¹¶ä¸”å¾—åˆ°ï¼Œ`_120_534`è¿™ä¸ªå‚æ•°æ˜¯æ’ä¿æŒä¸å˜çš„![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209102831102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
è‡³æ­¤ï¼Œåˆ†æå·®ä¸å¤šå°±ç»“æŸäº†ï¼Œä½ å¯ä»¥é€šè¿‡æ­£åˆ™ï¼Œåœ¨å›¾ä¹¦è¯¦æƒ…é¡µçš„urlä¸­åŒ¹é…å‡ºä»·æ ¼çš„urlçš„å‰ä¸‰ä¸ªå‚æ•°ï¼Œç„¶ååŠ ä¸Š`_120_534`è¿™ä¸ªå‚æ•°å°±æ˜¯æœ€ç®€çš„ä»·æ ¼urlã€‚
å¯æ˜¯ï¼Œè¿™é‡Œæˆ‘æ²¡ç”¨è¿™ç§æ–¹æ³•æ¥åšï¼Œæˆ‘å‘ç°åœ¨[å›¾ä¹¦ç›®å½•é¡µé¢](https://list.suning.com/1-502697-0.html)æ¯ä¸ªå›¾ä¹¦éƒ½æœ‰ä¸€ä¸ªå¦‚ä¸‹å›¾æ‰€ç¤ºçš„å‚æ•°ï¼Œæˆ‘ç›´æ¥ä»è¿™ä¸ªå‚æ•°ä¸­æå–çš„æ•°æ®![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209103558330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
**è‡³æ­¤ï¼Œä»·æ ¼çš„urlæå–å°±åˆ†æå®Œäº†ã€‚æ€»ç»“ä»·æ ¼çš„urlå°±æ˜¯ï¼š**
https://pas.suning.com/nspcsale_0_0000000 + (prdid) + _ + 0000000 + (prdid) + _ + (shopid) + _120_534.html?
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209103720249.png)
å¦å¤–ï¼Œæ•´ä¸ªé¡¹ç›®ä»…å®Œæˆäº†ä¸€å°éƒ¨åˆ†ï¼Œè¿˜æœ‰ä¸€äº›ç›¸å¯¹æ¯”è¾ƒéº»çƒ¦çš„æå–ï¼Œæ¯”å¦‚è¯´ï¼Œå›¾ä¹¦çš„åˆ†ç±»æå–ï¼Œæ¯ä¸ªå›¾ä¹¦éƒ½æœ‰ä¸€ä¸ªå¤§åˆ†ç±»ï¼Œå¤§åˆ†ç±»ä¸‹é¢æœ‰ç»†ä¸€ç‚¹çš„åˆ†ç±»ï¼Œç„¶åä¸‹é¢è¿˜æœ‰æ›´ç»†çš„åˆ†ç±»ï¼Œæœ€åæ‰æ˜¯å›¾ä¹¦ç›®å½•ã€‚è¿™é‡Œå°±ç›´æ¥æ”¾ä¸Šæˆ‘çš„ä»£ç äº†ï¼Œåˆ†æä¸æ˜¯å¤šä¹ˆå›°éš¾ï¼Œå°±æ˜¯æ¯”è¾ƒéº»çƒ¦è€Œå·²ï¼š
**tushu.py**

```python
# -*- coding: utf-8 -*-
import scrapy
import time
import re
from suningtushu.items import SuningtushuItem

count = 0


class TushuSpider(scrapy.Spider):
    name = 'tushu'
    allowed_domains = ['suning.com']
    start_urls = ['http://book.suning.com']

    def parse(self, response):
        list = response.xpath("//div[@class='menu-list']")
        div_list = list.xpath("./div[@class='menu-item']")

        '''æŠŠå¤§åˆ†ç±»æ”¾åˆ°åˆ—è¡¨ä¸­'''
        liname_list = []
        for div in div_list:
            liname_list.append(div.xpath("./dl/dt/h3/a/text()").extract_first())

        '''æ‰“å°å¯¹åº”çš„åˆ†ç±»ç›®å½•'''
        print("\nä¸€çº§åˆ†ç±»ï¼š", end="    ")
        for index in range(len(liname_list)):
            print("ã€{}ã€‘{}".format(index, liname_list[index]), end="  ")

        '''è·å–å¾…çˆ¬å–çš„ç›®å½•çš„ç´¢å¼•å€¼ã€éœ€è¦åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å­—ã€‘'''
        li_index = int(input("\n\nè¯·è¾“å…¥å¾…æŠ“å–çš„å›¾ä¹¦å‰é¢'ã€ã€‘'ä¸­å¯¹åº”çš„åºå·ï¼š\n"))
        div00 = list.xpath("./div[@class='menu-sub']")[li_index]
        p_list = div00.xpath("./div[1]/p")
        if p_list != []:
            '''æŠŠå¾…çˆ¬å–çš„pæ ‡ç­¾å¯¹åº”çš„é‡Œé¢çš„æ–‡æœ¬æ”¾åˆ° p_data_list ä¸­'''
            p_data_list = []
            for p in p_list:
                p_data_list.append(p.xpath("./a/text()").extract_first())
            '''åˆ—å‡ºå¾…çˆ¬å–çš„pæ ‡ç­¾çš„å°é¢˜ç›®'''
            print("\näºŒçº§åˆ†ç±»({})ï¼š".format(liname_list[li_index]), end="    ")
            for p_index in range(len(p_data_list)):
                print("ã€{}ã€‘{}".format(p_index, p_data_list[p_index]), end="  ")
            '''è¾“å…¥å¾…çˆ¬å–çš„äºŒçº§ç›®å½•ç´¢å¼•å€¼ã€éœ€è¦éªŒè¯æ˜¯å¦è¾“å…¥æ•°å­—ã€‘'''
            print("\n\nè¯·è¾“å…¥å¾…æŠ“å–çš„äºŒçº§ç›®å½• {}ç§ç±» çš„å›¾ä¹¦å‰é¢'ã€ã€‘'ä¸­å¯¹åº”çš„åºå·ï¼š\r".format(liname_list[li_index]))
            pi_index = int(input())
            '''è·å–å¤‡çˆ¬å–çš„äºŒçº§ç›®å½•å¯¹åº”ç´¢å¼•çš„å›¾ä¹¦'''
            ul_list = div00.xpath("./div[1]/ul")
            ul_li_list = ul_list[pi_index].xpath("./li")
            '''éå†liï¼Œå–å‡ºå¯¹åº”çš„é“¾æ¥ä»¥åŠåå­—ï¼Œå­˜åˆ°å­—å…¸é‡Œ'''
            ul_li_dict_list = []
            for li in ul_li_list:
                li_dict = {}
                li_dict['name'] = li.xpath("./a/text()").extract_first()
                li_dict['href'] = li.xpath("./a/@href").extract_first()
                ul_li_dict_list.append(li_dict)
            '''å–å¾—åˆ—è¡¨ä¸­çš„å­—å…¸ï¼Œè¾“å‡ºä¸‰çº§ç›®å½•'''
            print("\nä¸‰çº§åˆ†ç±»({})ï¼š".format(p_data_list[pi_index]), end="    ")
            for ul_li_dict_index in range(len(ul_li_dict_list)):
                print("ã€{}ã€‘{}".format(ul_li_dict_index, ul_li_dict_list[ul_li_dict_index]['name']), end="  ")
            '''è¾“å…¥ä¸‰çº§ç›®å½•å¾…æŠ“å–çš„å›¾ä¹¦çš„åˆ†ç±»çš„ç´¢å¼•ã€éœ€è¦éªŒè¯æ˜¯å¦è¾“å…¥æ•°å­—ã€‘'''
            print("\n\nè¯·è¾“å…¥å¾…æŠ“å–çš„ä¸‰çº§ç›®å½• {}ç§ç±» çš„å›¾ä¹¦å‰é¢'ã€ã€‘'ä¸­å¯¹åº”çš„åºå·:\r".format(p_data_list[pi_index]))
            ul_index = int(input())

            '''Xpathæ•ˆç‡æ¥è¯´æ¯”ä¸ä¸Šæ­£åˆ™ï¼Œä½†æ˜¯ç¨³å®šï¼Œæ‰€ä»¥è¿™ä¸ªäº‹å§ï¼Œæˆ‘ä¸€éƒ¨åˆ†ç”¨çš„æ­£åˆ™ï¼Œä¸€éƒ¨åˆ†ç”¨çš„xpath'''
            '''è¾“å…¥å®Œä»¥åï¼Œéœ€è¦å–å‡ºå¯¹åº”çš„å­—å…¸ä¸­çš„hrefï¼Œè¿›è¡Œåˆå§‹é¡µé¢çš„è¯·æ±‚'''
            data_href = ul_li_dict_list[ul_index]['href']
            # https://list.suning.com/1-502314-0.html
            '''ä¸‹ä¸€é¡µæ ¹æ®'''
            data = re.findall(r"https://list.suning.com/1-(.*)-0.html", data_href)[0]

            '''è¯·æ±‚ä¸€çº§å›¾ä¹¦é¡µé¢'''
            yield scrapy.Request(
                data_href,
                callback=self.seconed_html_request,
                meta={"data": data}
            )
        else:
            '''è·å–å¤‡çˆ¬å–çš„äºŒçº§ç›®å½•å¯¹åº”ç´¢å¼•çš„å›¾ä¹¦'''
            ul_list = div00.xpath("./div[1]/ul")
            ul_li_list = ul_list.xpath("./li")
            # print(ul_li_list)
            '''éå†liï¼Œå–å‡ºå¯¹åº”çš„é“¾æ¥ä»¥åŠåå­—ï¼Œå­˜åˆ°å­—å…¸é‡Œ'''
            ul_li_dict_list = []
            for li in ul_li_list:
                li_dict = {}
                li_dict['name'] = li.xpath("./a/text()").extract_first()
                li_dict['href'] = li.xpath("./a/@href").extract_first()
                ul_li_dict_list.append(li_dict)
            # print(ul_li_dict_list)
            '''å–å¾—åˆ—è¡¨ä¸­çš„å­—å…¸ï¼Œè¾“å‡ºå›¾ä¹¦ç§ç±»ç›®å½•'''
            for ul_li_dict_index in range(len(ul_li_dict_list)):
                print("ã€{}ã€‘{}".format(ul_li_dict_index, ul_li_dict_list[ul_li_dict_index]['name']), end="  ")
            '''è¾“å…¥ä¸‰çº§ç›®å½•å¾…æŠ“å–çš„å›¾ä¹¦çš„åˆ†ç±»çš„ç´¢å¼•ã€éœ€è¦éªŒè¯æ˜¯å¦è¾“å…¥æ•°å­—ã€‘'''
            print("\n\nè¯·è¾“å…¥å¾…æŠ“å–çš„å›¾ä¹¦ç§ç±»å‰é¢'ã€ã€‘'ä¸­å¯¹åº”çš„åºå·:\r")
            ul_index = int(input())

            '''Xpathæ•ˆç‡æ¥è¯´æ¯”ä¸ä¸Šæ­£åˆ™ï¼Œä½†æ˜¯ç¨³å®šï¼Œæ‰€ä»¥è¿™ä¸ªäº‹å§ï¼Œæˆ‘ä¸€éƒ¨åˆ†ç”¨çš„æ­£åˆ™ï¼Œä¸€éƒ¨åˆ†ç”¨çš„xpath'''
            '''è¾“å…¥å®Œä»¥åï¼Œéœ€è¦å–å‡ºå¯¹åº”çš„å­—å…¸ä¸­çš„hrefï¼Œè¿›è¡Œåˆå§‹é¡µé¢çš„è¯·æ±‚'''
            data_href = ul_li_dict_list[ul_index]['href']
            # https://list.suning.com/1-502314-0.html
            '''ä¸‹ä¸€é¡µæ ¹æ®'''
            data = re.findall(r"https://list.suning.com/1-(.*)-0.html", data_href)[0]

            '''è¯·æ±‚ä¸€çº§å›¾ä¹¦é¡µé¢'''
            yield scrapy.Request(
                data_href,
                callback=self.seconed_html_request,
                meta={"data": data}
            )

    def seconed_html_request(self, response):
        li_list = response.xpath("//ul[@class='clearfix']/li")
        '''list_alå†…éƒ¨å­˜æ”¾çš„æœ‰ æ¯ä¸ªå›¾ä¹¦çš„ 'prdid','shopid' ä»¥åŠå›¾ä¹¦ä¸å®Œæ•´çš„urlåœ°å€'''
        list_all = []
        for li in li_list:
            '''åŒ¹é…å›¾ä¹¦çš„urlä»¥åŠå›¾ä¹¦ä»·æ ¼urlçš„é‡è¦å‚æ•°'''
            li_list = li.xpath(".//*").re(
                '''sa-data="{'eletp':'prd','prdid':'(.*?)','shopid':'(.*?)'.*?href="(.*?)" name''')[
                      0:3]
            '''æå–çš„è¿‡ç¨‹ä¸­ï¼Œæ‹¿ç€ 'prdid','shopid' ä»¥åŠå›¾ä¹¦ä¸å®Œæ•´çš„urlåœ°å€å»è¯·æ±‚ç¬¬ä¸‰ä¸ªé¡µé¢ï¼Œå–å¾—æ‰€æœ‰æ•°æ®'''
            url = "https:" + li_list[-1]
            yield scrapy.Request(
                url,
                callback=self.third_html_request,
                meta={"data": li_list}
            )
            list_all.append(li_list)
        # print(len(list_all))
        '''ä¸‹ä¸€é¡µè¯·æ±‚'''
        res = re.findall('''title="(ä¸‹ä¸€é¡µ)"''', response.text)[0]
        if res == 'ä¸‹ä¸€é¡µ':
            global count
            count = count + 1
            nex_url = "https://list.suning.com/1-" + response.meta['data'] + "-{}".format(
                count) + "-0-0-0-0-14-0-4.html"
            print(nex_url)
            yield scrapy.Request(
                nex_url,
                callback=self.seconed_html_request,
                meta={"data": response.meta['data']}
            )

    def third_html_request(self, response):
        '''ç¬¬ä¸‰é¡µè¿›è¡Œæ•°æ®çš„æå–ä»¥åŠå­˜æ”¾'''
        li_list = response.meta["data"]
        item = SuningtushuItem()
        '''Book_Srcæå–'''
        item['Book_Src'] = response.url
        item['Title'] = response.xpath("//h1/text()").extract()[1]
        rbq_list = response.xpath("//ul[@class='bk-publish clearfix']/li")

        '''ä¿®è¡¥çš„bug'''
        count = len(rbq_list)
        try:
            if count > 0:
                '''ä½œè€…'''
                item['Author'] = rbq_list[0].xpath('./text()').extract_first()
            if count > 1:
                '''å‡ºç‰ˆç¤¾'''
                item['Press'] = rbq_list[1].xpath('./text()').extract_first()
            if count > 2:
                '''å‡ºç‰ˆæ—¥æœŸ'''
                item['Publish_Time'] = rbq_list[2].xpath('./span[2]/text()').extract_first()
        except:
            item['Author'] = " "
            item['Press'] = " "
            item['Publish_Time'] = " "
            print("å¼‚å¸¸å›¾ä¹¦ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚")
        # print(item)
        '''æ„é€ ä»·æ ¼è¯·æ±‚çš„url'''
        url = "https://pas.suning.com/nspcsale_0_0000000" + li_list[0] + "_0000000" + li_list[0] + "_" + li_list[
            1] + "_120_534.html?"
        # print(url)
        yield scrapy.Request(
            url,
            callback=self.Fourth_html_request,
            meta={"item": item}
        )

    def Fourth_html_request(self, response):
        item = response.meta['item']
        '''è¿›è¡Œå›¾ä¹¦ä»·æ ¼çš„æå–'''
        text = response.text
        '''åŸè°…æˆ‘ï¼Œå¯¹é€Ÿåº¦æ— èƒ½ä¸ºåŠ›äº†ï¼Œæœ¬æ¥ä»·æ ¼è¿™ä¸ªè·å–çš„æ—¶å€™è§£æä¸»æœºå°±å¾ˆæ…¢çš„'''
        item['Price'] = re.findall('''"netPrice":"(.*?)"''', text)[0]
        yield item

```
è¿™é‡Œï¼Œæ˜¯çˆ¬è™«æ–‡ä»¶ã€‚å¯¹äºæœ‰äº›ä¹¦ç±ï¼Œä½œè€…ä¿¡æ¯ä»¥åŠå‡ºç‰ˆç¤¾ï¼Œå‡ºç‰ˆæ—¥æœŸå­˜æ”¾çš„ä½ç½®æœ‰ç‚¹è¿·ï¼Œå› ä¸ºæ— æ³•åˆ¤æ–­å®ƒæ˜¯ä»€ä¹ˆç»“æ„ï¼Œæœ‰çš„å›¾ä¹¦æœ‰ä½œè€…ä¿¡æ¯ï¼Œæœ‰çš„æ²¡æœ‰ï¼Œæœ‰çš„å›¾ä¹¦è¯¦æƒ…é¡µè¿˜æŠŠä¿¡æ¯æ”¾åˆ°äº†æ—®æ—¯é‡Œï¼Œç¡®å®ï¼Œå¯¹äºä½œè€… / å‡ºç‰ˆç¤¾ / å‡ºç‰ˆæ—¥æœŸè¿™ä¸ªåœ°æ–¹ä»…æœ‰çš„å‡ æœ¬ä¹¦ä½œè€…ä»¥åŠå‡ºç‰ˆä¿¡æ¯çš„bugï¼Œæˆ‘é‡‡å–äº†ç›´æ¥å°†ä½œè€… / å‡ºç‰ˆç¤¾ / å‡ºç‰ˆæ—¥æœŸç½®ä¸ºç©ºä¸²æ¥è§£å†³ã€‚æ‰€ä»¥ï¼ŒåŸè°…æˆ‘ç¡®å®æ²¡èŠ±å¤ªå¤šçš„ç²¾åŠ›å»è§£å†³è¿™ä¸ªç¼ºé™·ã€‚
**pipeline.py**
```python
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
from pymongo import MongoClient
import re


class SuningtushuPipeline(object):
    def open_spider(self, spider):
        print("Mongodb is establishing a connection....................")
        client = MongoClient()
        self.connect = client['suning']['data']
        print("===================================================================================")
        print("\n---------Name of the project: è‹å®æ˜“è´­å›¾ä¹¦çˆ¬è™«---------------------              ||\n")
        print("---------author: Caiden_Micheal                                                  ||")
        print("---------GitHub address: https://github.com/Meterprete?tab=repositories          ||")
        print("---------Personal mailbox: wangxinqhou@foxmail.com                               ||")
        print("---------time: 2020.2.7                                                          ||\n")
        print("===================================================================================")

    def process_item(self, item, spider):
        self.data_clear(item)
        m = self.connect.insert(dict(item))
        print(m)
        return item

    def data_clear(self, contant):
        '''ç®€å•çš„æ•°æ®æ¸…æ™°'''
        re_compile = re.compile("\s|\r|\n|\t")
        try:
            contant['Author'] = re_compile.sub("", contant['Author'])
            contant['Press'] = re_compile.sub("", contant['Press'])
        except Exception as e:
            print(e)
        contant['Title'] = re_compile.sub("", contant['Title'])
        return contant

```
æ•´ä¸ªé¡¹ç›®å°±è¿™æ ·ç»“æŸäº†ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿è¡Œæˆªå›¾ï¼š
**åˆå§‹åŒ–ç¨‹åºæ—¶çš„æˆªå›¾**
è¿™é‡Œå¦‚æœä½ æƒ³æŠŠå®ƒæ”¹æˆæ— è¾“å…¥æŠ“å–æ‰€æœ‰åˆ†ç±»çš„è¿™æ ·çš„ä¸€ä¸ªçˆ¬è™«ç¨‹åºä¹Ÿæ¯”è¾ƒå¥½å®ç°ï¼Œåªéœ€è¦å¾ªç¯åˆ—è¡¨æå–åˆ†ç±»ä¿¡æ¯å°±è¡Œï¼Œè¿™é‡Œåœ¨ç¨‹åºä¸­æ˜¯å¯ä¿®æ”¹çš„ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209105423266.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
ä¸‹é¢æ˜¯è¿è¡Œè¿‡ç¨‹ä¸­æ‰“å°å‡ºæ¥çš„ä¸‹ä¸€é¡µçš„urlä»¥åŠæ•°æ®ä¿å­˜åˆ°Mongodbä»¥åè¿”å›çš„ä¿¡æ¯ï¼š![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2020020911004436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
æˆ‘ä»¬å»Mongodbä¸­çœ‹ä¸€çœ‹æ•°æ®æŠ“å–æƒ…å†µï¼Œè¿™é‡Œå°±è®©å®ƒæŠ“äº†1358æ¡æ•°æ®å°±å¼ºåˆ¶åœæ­¢äº†ç¨‹åºï¼ŒåŸºæœ¬ä¸Šæ¯ç§’é’Ÿæå–ä¸€äºŒåæ¡æ•°æ®çš„æ ·å­ï¼Œæ²¡åšå…·ä½“çš„ç»Ÿè®¡ï¼Œé€Ÿåº¦è¿˜æ˜¯æŒºå¿«çš„ï¼Œè¿™é‡Œå”¯ä¸€çš„ä¸æ»¡å°±æ˜¯è‹å®ä»·æ ¼urlè¿”å›é€Ÿåº¦ç¡®å®æœ‰ç‚¹æ…¢ï¼Œè¿™ä¸ªå’Œäººå®¶æœåŠ¡å™¨æœ‰å…³ï¼Œå’±ä¹Ÿæ²¡åŠæ³•ï¼Œä¸è¿‡Scrapyæ­¤æ—¶çš„å¼‚æ­¥ç½‘ç»œæ¡†æ¶å°±æœ‰äº†æ–½å±•æ‰åçš„åœ°æ–¹äº†ï¼Œä»¤æ’ä¸€å¥~~ä½ ğŸç‚¸äº†~~ æ•°æ®æå–æŒºçº¯å‡€çš„ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200209110547352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ0OTUxOA==,size_16,color_FFFFFF,t_70)
